{"cells":[{"cell_type":"code","source":["# ===================== Phase 0 — CONFIG & IMPORTS =====================\n","\n","# Destination table in the Lakehouse\n","TARGET_TABLE = \"tbl_bronze_weather_raw\"\n","\n","# Daily variables to be requested from Open-Meteo\n","DAILY_VARS = \"temperature_2m_max,temperature_2m_min,precipitation_sum,wind_speed_10m_max\"\n","TZ         = \"Europe/Lisbon\"\n","\n","# HTTP / retries\n","HTTP_TIMEOUT = 300\n","MAX_WORKERS  = 2\n","RETRY_TRIES  = 6\n","RETRY_BASE_S = 1.0\n","JITTER_S     = 0.5\n","\n","# Batching & logging\n","BATCH_SIZE     = 24\n","COOLDOWN_S     = 2\n","PROGRESS_EVERY = 5\n","\n","# Precision for lat/lon (aligned with dim_location_silver)\n","DECIMAL_SCALE     = 5\n","# up to 3 integer digits (e.g. 180) + decimal places\n","DECIMAL_PRECISION = 3 + DECIMAL_SCALE\n","\n","# Canonical column order before creating the Spark DataFrame\n","COL_ORDER = [\n","    \"date\",\n","    \"temperature_2m_max\",\n","    \"temperature_2m_min\",\n","    \"precipitation_sum\",\n","    \"wind_speed_10m_max\",\n","    \"latitude\",\n","    \"longitude\",\n","    \"parish\"\n","]\n","\n","# --- imports ---\n","from pyspark.sql import functions as F, types as T\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import requests, pandas as pd, numpy as np, math, time, random\n","from requests.adapters import HTTPAdapter\n","try:\n","    from urllib3.util import Retry\n","except Exception:\n","    from urllib3.util.retry import Retry\n","from datetime import datetime\n","\n","\n","def ensure_col_order_and_types(pdf: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Ensure the DataFrame has all columns in COL_ORDER with proper basic types.\"\"\"\n","    if \"date\" in pdf.columns:\n","        pdf[\"date\"] = pd.to_datetime(pdf[\"date\"], errors=\"coerce\").dt.date\n","\n","    for c in COL_ORDER:\n","        if c not in pdf.columns:\n","            pdf[c] = np.nan\n","\n","    return pdf[COL_ORDER]\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:39.6178421Z","session_start_time":null,"execution_start_time":"2025-11-25T20:58:39.619058Z","execution_finish_time":"2025-11-25T20:58:39.9217196Z","parent_msg_id":"9d345817-e482-4dfa-9ccd-c1e7fafb30dc"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9fb4db06-f7fc-4c35-9d2d-b82fb1387f95"},{"cell_type":"code","source":["# ===================== Phase 1 — Date window (DIM_DATE) =====================\n","dim_date = (\n","    spark.read.table(\"dim_date_silver\")\n","    .select(F.col(\"Registration Date\").cast(\"date\").alias(\"date\"))\n",")\n","\n","minmax = (\n","    dim_date\n","    .agg(F.min(\"date\").alias(\"start\"), F.max(\"date\").alias(\"end\"))\n","    .collect()[0]\n",")\n","\n","if minmax[\"start\"] is None or minmax[\"end\"] is None:\n","    raise ValueError(\"dim_date_silver does not contain any values in 'Registration Date'.\")\n","\n","start_date = minmax[\"start\"].strftime(\"%Y-%m-%d\")\n","end_date   = minmax[\"end\"].strftime(\"%Y-%m-%d\")\n","\n","N_DAYS = (\n","    datetime.strptime(end_date, \"%Y-%m-%d\")\n","    - datetime.strptime(start_date, \"%Y-%m-%d\")\n",").days + 1\n","\n","print(f\"Date window in dim_date_silver: {start_date} → {end_date} (inclusive) | N_DAYS={N_DAYS}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:39.7043113Z","session_start_time":null,"execution_start_time":"2025-11-25T20:58:39.9239439Z","execution_finish_time":"2025-11-25T20:58:41.38705Z","parent_msg_id":"f350293a-f996-4275-a799-c40e58b55950"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Date window in dim_date_silver: 2020-01-01 → 2024-12-31 (inclusive) | N_DAYS=1827\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bae7a354-8a1e-4047-9b8b-4c3e7f678339"},{"cell_type":"code","source":["# ===================== Phase 2 — Parish coordinates (centroids) =====================\n","dim_loc_raw = spark.read.table(\"dim_location_silver\")\n","\n","# Schema validation (1 row per parish with these fields)\n","cols = {field.name for field in dim_loc_raw.schema.fields}\n","required = {\"Parish Name\", \"Latitude_Centroid\", \"Longitude_Centroid\"}\n","\n","if not required.issubset(cols):\n","    raise ValueError(\n","        \"dim_location_silver must contain the columns: \"\n","        \"'Parish Name', 'Latitude_Centroid', 'Longitude_Centroid'.\"\n","    )\n","\n","# Normalisation and typing\n","dl = (\n","    dim_loc_raw\n","    .select(\n","        F.upper(F.trim(F.col(\"Parish Name\"))).alias(\"parish\"),\n","        F.col(\"Latitude_Centroid\").cast(\"double\").alias(\"lat\"),\n","        F.col(\"Longitude_Centroid\").cast(\"double\").alias(\"lon\")\n","    )\n","    .dropna(subset=[\"parish\", \"lat\", \"lon\"])\n","    .dropDuplicates([\"parish\"])\n",")\n","\n","# DISTINCT list of parishes\n","TARGET_PARISHES = [\n","    row[\"parish\"] for row in dl.select(\"parish\").dropDuplicates().collect()\n","]\n","print(f\"Detected parishes: {len(TARGET_PARISHES)}\")\n","\n","# Optional whitelist\n","# WHITELIST = [\"ALVALADE\", \"ARROIOS\", ...]\n","# TARGET_PARISHES = [\n","#     p for p in TARGET_PARISHES\n","#     if p in {x.upper().strip() for x in WHITELIST}\n","# ]\n","dl = dl.where(F.col(\"parish\").isin(TARGET_PARISHES))\n","\n","# Prepare list of coordinates\n","coords = [(row[\"lat\"], row[\"lon\"], row[\"parish\"]) for row in dl.collect()]\n","\n","if not coords:\n","    raise ValueError(\n","        \"dim_location_silver returned no coordinates \"\n","        \"(check parish field names and values).\"\n","    )\n","\n","print(\n","    f\"Parishes to process: {len(coords)} | \"\n","    f\"Expected rows ≈ {len(coords) * N_DAYS:,}\"\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:39.7962183Z","session_start_time":null,"execution_start_time":"2025-11-25T20:58:41.3890513Z","execution_finish_time":"2025-11-25T20:58:42.879318Z","parent_msg_id":"6e8ddb44-7767-4718-88a2-2ef185ff7e2f"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Detected parishes: 24\nParishes to process: 24 | Expected rows ≈ 43,848\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62472c63-0016-4afe-a2d5-a790539f8d17"},{"cell_type":"code","source":["# ===================== Phase 3 — HTTP session + Open-Meteo fetch =====================\n","session = requests.Session()\n","retry_cfg = Retry(\n","    total=RETRY_TRIES,\n","    backoff_factor=RETRY_BASE_S,\n","    status_forcelist=[429, 500, 502, 503, 504],\n","    allowed_methods=[\"GET\"],\n","    respect_retry_after_header=True\n",")\n","adapter = HTTPAdapter(\n","    max_retries=retry_cfg,\n","    pool_connections=MAX_WORKERS,\n","    pool_maxsize=MAX_WORKERS\n",")\n","session.mount(\"https://\", adapter)\n","session.headers.update({\"User-Agent\": \"fabric-notebook-open-meteo/1.0\"})\n","\n","BASE_URL = \"https://archive-api.open-meteo.com/v1/archive\"\n","\n","\n","def fetch_daily(lat: float, lon: float, parish: str) -> pd.DataFrame:\n","    \"\"\"\n","    Calls Open-Meteo for a given (lat, lon) and returns a pandas DataFrame\n","    with columns in COL_ORDER.\n","    \"\"\"\n","    if lat is None or lon is None or math.isnan(lat) or math.isnan(lon):\n","        raise ValueError(\"Invalid coordinates.\")\n","\n","    params = dict(\n","        latitude=lat,\n","        longitude=lon,\n","        daily=DAILY_VARS,\n","        start_date=start_date,\n","        end_date=end_date,\n","        timezone=TZ\n","    )\n","\n","    for i in range(RETRY_TRIES):\n","        try:\n","            # Small random jitter to avoid thundering herd\n","            time.sleep(JITTER_S * random.random())\n","\n","            r = session.get(BASE_URL, params=params, timeout=HTTP_TIMEOUT)\n","\n","            # Handle rate limiting with respect to Retry-After header\n","            if r.status_code == 429:\n","                ra = r.headers.get(\"Retry-After\")\n","                wait = (\n","                    int(ra) if ra and ra.isdigit()\n","                    else (RETRY_BASE_S * (2 ** i))\n","                ) + JITTER_S * random.random()\n","                time.sleep(wait)\n","                continue\n","\n","            # Retry on 5xx\n","            if r.status_code >= 500:\n","                raise RuntimeError(f\"HTTP {r.status_code}: {r.text[:300]}\")\n","\n","            # Treat any non-200 (other than 429 above) as an error\n","            if r.status_code != 200:\n","                raise RuntimeError(f\"HTTP {r.status_code}: {r.text[:300]}\")\n","\n","            j = r.json()\n","            if \"daily\" not in j or \"time\" not in j[\"daily\"]:\n","                raise RuntimeError(\"Response does not contain 'daily.time'.\")\n","\n","            d = j[\"daily\"]\n","            pdf = pd.DataFrame(d)\n","\n","            if pdf.empty:\n","                pdf = pd.DataFrame(columns=COL_ORDER)\n","            else:\n","                pdf[\"date\"] = pd.to_datetime(pdf[\"time\"], errors=\"coerce\").dt.date\n","                pdf.drop(columns=[\"time\"], inplace=True, errors=\"ignore\")\n","\n","                pdf[\"latitude\"] = float(lat)\n","                pdf[\"longitude\"] = float(lon)\n","                pdf[\"parish\"] = parish\n","\n","                for c in [\n","                    \"temperature_2m_max\",\n","                    \"temperature_2m_min\",\n","                    \"precipitation_sum\",\n","                    \"wind_speed_10m_max\"\n","                ]:\n","                    if c in pdf.columns:\n","                        pdf[c] = pd.to_numeric(pdf[c], errors=\"coerce\")\n","\n","            return ensure_col_order_and_types(pdf)\n","\n","        except Exception as e:\n","            if i < RETRY_TRIES - 1:\n","                # Exponential backoff with jitter\n","                time.sleep(RETRY_BASE_S * (2 ** i) + JITTER_S * random.random())\n","            else:\n","                raise RuntimeError(\n","                    f\"Open-Meteo request failed after {RETRY_TRIES} attempts | \"\n","                    f\"parish={parish}, lat={lat}, lon={lon}, \"\n","                    f\"start={start_date}, end={end_date} | error={e}\"\n","                )\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:40.0317016Z","session_start_time":null,"execution_start_time":"2025-11-25T20:58:42.8817231Z","execution_finish_time":"2025-11-25T20:58:43.1676949Z","parent_msg_id":"8aa7ea5c-a46f-4681-8a19-e34f7cf8a048"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e1279e42-8c2d-4b8a-bbe2-88444c12b1c3"},{"cell_type":"code","source":["# ===================== Phase 4 — Batched execution with progress =====================\n","frames = []\n","total = len(coords)\n","completed = 0\n","rows_done = 0\n","expected_rows = total * N_DAYS\n","\n","print(\n","    f\"Start: {total} coordinates | threads={MAX_WORKERS} | \"\n","    f\"batch={BATCH_SIZE} | timeout={HTTP_TIMEOUT}s\"\n",")\n","t0 = time.time()\n","\n","for start in range(0, total, BATCH_SIZE):\n","    batch = coords[start:start + BATCH_SIZE]\n","    failures = []\n","\n","    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n","        futures = {\n","            ex.submit(fetch_daily, lat, lon, parish): (lat, lon, parish)\n","            for (lat, lon, parish) in batch\n","        }\n","\n","        for fut in as_completed(futures):\n","            lat, lon, parish = futures[fut]\n","            try:\n","                pdf = fut.result()\n","                if not pdf.empty:\n","                    frames.append(pdf)\n","                    # Approximately N_DAYS rows per coordinate\n","                    rows_done += len(pdf)\n","            except Exception as e:\n","                failures.append((lat, lon, parish, str(e)))\n","\n","            completed += 1\n","            if completed % PROGRESS_EVERY == 0:\n","                elapsed = int(time.time() - t0)\n","                pct_coords = 100.0 * completed / total\n","                pct_rows = (\n","                    100.0 * rows_done / expected_rows\n","                    if expected_rows\n","                    else 0.0\n","                )\n","                eta_s = int(\n","                    elapsed * (total / max(1, completed) - 1)\n","                )\n","                print(\n","                    f\"[progress] coords {completed}/{total} \"\n","                    f\"({pct_coords:.1f}%) | \"\n","                    f\"rows {rows_done}/{expected_rows} \"\n","                    f\"({pct_rows:.1f}%) | \"\n","                    f\"batch_failures={len(failures)} | \"\n","                    f\"elapsed={elapsed}s | eta~{eta_s}s\"\n","                )\n","\n","    time.sleep(COOLDOWN_S)\n","\n","if not frames:\n","    raise RuntimeError(\n","        \"No data returned by Open-Meteo \"\n","        \"(check date window and coordinates).\"\n","    )\n","\n","all_pdf = pd.concat(frames, ignore_index=True)\n","print(f\"Total records retrieved: {len(all_pdf):,}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:40.3004992Z","session_start_time":null,"execution_start_time":"2025-11-25T20:58:43.1698285Z","execution_finish_time":"2025-11-25T20:59:21.2342131Z","parent_msg_id":"1fe5f37f-debf-4240-bb67-878346f14be1"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Start: 24 coordinates | threads=2 | batch=24 | timeout=300s\n[progress] coords 5/24 (20.8%) | rows 9135/43848 (20.8%) | batch_failures=0 | elapsed=1s | eta~3s\n[progress] coords 10/24 (41.7%) | rows 18270/43848 (41.7%) | batch_failures=0 | elapsed=2s | eta~2s\n[progress] coords 15/24 (62.5%) | rows 27405/43848 (62.5%) | batch_failures=0 | elapsed=32s | eta~19s\n[progress] coords 20/24 (83.3%) | rows 36540/43848 (83.3%) | batch_failures=0 | elapsed=33s | eta~6s\nTotal records retrieved: 43,848\n"]}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cabc3d8d-4bb0-4995-89e9-5982177a3d54"},{"cell_type":"code","source":["# ===================== Phase 5 — FULL LOAD (staging, without surrogate keys) =====================\n","schema = T.StructType([\n","    T.StructField(\"date\",               T.DateType(),   False),\n","    T.StructField(\"temperature_2m_max\", T.DoubleType(), True),\n","    T.StructField(\"temperature_2m_min\", T.DoubleType(), True),\n","    T.StructField(\"precipitation_sum\",  T.DoubleType(), True),\n","    T.StructField(\"wind_speed_10m_max\", T.DoubleType(), True),\n","    T.StructField(\"latitude\",           T.DoubleType(), True),\n","    T.StructField(\"longitude\",          T.DoubleType(), True),\n","    T.StructField(\"parish\",             T.StringType(), True)\n","])\n","\n","# Align columns and create base DataFrame\n","all_pdf = ensure_col_order_and_types(all_pdf.copy())\n","\n","df_out = (\n","    spark.createDataFrame(all_pdf[COL_ORDER], schema=schema)\n","         .withColumn(\"parish\", F.upper(F.trim(F.col(\"parish\"))))\n","         # Store rounded lat/lon for audit/troubleshooting; they are not keys\n","         .withColumn(\n","             \"latitude\",\n","             F.round(F.col(\"latitude\"), DECIMAL_SCALE).cast(\n","                 T.DecimalType(DECIMAL_PRECISION, DECIMAL_SCALE)\n","             )\n","         )\n","         .withColumn(\n","             \"longitude\",\n","             F.round(F.col(\"longitude\"), DECIMAL_SCALE).cast(\n","                 T.DecimalType(DECIMAL_PRECISION, DECIMAL_SCALE)\n","             )\n","         )\n","         .dropna(subset=[\"date\", \"parish\"])\n","         .dropDuplicates([\"date\", \"parish\"])\n",")\n","\n","# Full overwrite (no partitions). Surrogate keys and joins are handled later in the Dataflow.\n","(df_out.write\n","     .format(\"delta\")\n","     .mode(\"overwrite\")\n","     .option(\"overwriteSchema\", \"true\")\n","     .saveAsTable(TARGET_TABLE))\n","\n","print(f\"FULL LOAD completed: {TARGET_TABLE} (rows: {df_out.count():,})\")\n","\n","# Optional: logical clustering by date to speed up temporal filters\n","spark.sql(f\"OPTIMIZE {TARGET_TABLE} ZORDER BY (date)\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"ec19c72b-9cbb-4050-9871-321cfb1811ea","normalized_state":"finished","queued_time":"2025-11-25T20:58:40.4743741Z","session_start_time":null,"execution_start_time":"2025-11-25T20:59:21.2364113Z","execution_finish_time":"2025-11-25T20:59:40.7626622Z","parent_msg_id":"c8fc9832-e5cd-487f-aa47-5e8a42622c28"},"text/plain":"StatementMeta(, ec19c72b-9cbb-4050-9871-321cfb1811ea, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["FULL LOAD completed: tbl_bronze_weather_raw (rows: 43,848)\n"]},{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,numFilesUpdatedWithoutRewrite:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesUpdatedWithoutRewrite:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemovedBreakdown:array<struct<reason:string,metrics:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>>>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8b335b20-b96f-4f78-99c7-3f2961105197"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"3bd3ae8f-fc8d-4233-80cd-88b846c96078"}],"default_lakehouse":"3bd3ae8f-fc8d-4233-80cd-88b846c96078","default_lakehouse_name":"LH_SOURCES_NA_MINHA_RUA","default_lakehouse_workspace_id":"235ed27f-bd32-43a9-93a3-cd913c6d292c"}}},"nbformat":4,"nbformat_minor":5}